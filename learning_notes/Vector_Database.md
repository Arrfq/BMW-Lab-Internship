> Michael Harditya (TEEP)
# Vector Database Review Summary
Vector database is a kind of database that stores vector rather than entries of data. These vectors are great for defining relation, also enhance matching algorithm to process queries. It promotes the usage of vector embeddings, especially in NLP since it uses embeddings.
## **Table of Contents**
- [Vector Database Review Summary](#vector-database-review-summary)
  - [**Table of Contents**](#table-of-contents)
  - [Vector Database](#vector-database)
  - [Vector Embeddings](#vector-embeddings)
  - [FAISS](#faiss)
## Vector Database
A vector database is a database that store vectors of data. These vectors are usually generated by a [Vector Embedding](#vector-embeddings) mechanism that translates data into it's representative vector (usually in numerical values). The common pipeline of a vector database is:
1. Indexing, the vector database indexes the vectors using an algorithm, to maps the vectors to a data structure for faster searching. Index and the vectors then saved into the vector database ready to be queried.
2. Querying, happens when someone wanted to fetch an information by sending a query. The query then translated into an indexed query vector, then all the vectors stored are searched by using nearest-neighbors algorithm.
3. Post Processing, the database retrieves the final nearest neighbors from the dataset, post-processes, and returns the final results. It include re-ranking the nearest neighbors using different similarity algorithm.

Some of vector databases available and open-sourced are:
- [Chroma](https://www.trychroma.com/)
- [FAISS](https://github.com/facebookresearch/faiss)
- [Weaviate](https://weaviate.io/)
- [Qdrant](https://qdrant.tech/)
  
## Vector Embeddings
Vector Embeddings are numerical representations of data that captures semantic relationships and similarities, making it possible to be used in mathematical operations and comparison. There are many existing vector embeddings algorithm and codes, usually built for specific use case. For example, Word2Vec, maps words with other words to find its relation by learning texts that consists the words.
  
There are many types of embeddings, differentiated by the media it process. Some of them are:
1. Text Embeddings, the easiest one to understand and used in most NLP models. It structures a text based on its relation to other words.
2. Image Embeddings, representing multiple features of an image. It can be full images, individual pixels, a group of pixels, filtered images, and so on.
3. Document Embeddings, extends text embeddings by capturing the overall semantic meaning of the whole document, make it possible to do tasks like document classification, clustering, or information retrieval.

## FAISS
FAISS is Facebook AI Similarity Search, developed by Facebook AI that enables efficient similarity search. It is open-source and can be used directly to Python codes. It provides several distance algorithm when building vectors and finding relations between the data points. One of them is L2 (Euclidean) distance, an algorithm that is very accurate in finding relation between the point that we want to search (in this term, a query) with the whole data in the vector space. Below is L2 distance in mathematical expression, y is data point of the database, q is the query. This distance is very time consuming and heavy load if the data saved in the vector space is big.

$d = \sqrt{\sum_{i=1}^{k} (y_i - q_i)^2}$

To enhance the similarity search, FAISS use partitioning of the vector spaces. By partitioning, the query is processed only to the closest partition only, not the whole vector space, make it faster to be processed.

For more integration of FAISS can be found here: [FAISS Tutorial](https://www.pinecone.io/learn/series/faiss/faiss-tutorial/).