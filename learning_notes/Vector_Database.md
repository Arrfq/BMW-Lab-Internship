> Michael Harditya (TEEP)
# Vector Database Review Summary
Vector database is a kind of database that stores vector rather than entries of data. These vectors are great for defining relation, also enhance matching algorithm to process queries. It promotes the usage of vector embeddings, especially in NLP since it uses embeddings.
## **Table of Contents**
- [Vector Database Review Summary](#vector-database-review-summary)
  - [**Table of Contents**](#table-of-contents)
  - [Vector Database](#vector-database)
  - [Vector Embeddings](#vector-embeddings)
  - [FAISS](#faiss)
  - [List of Possible Implementation for the Project](#list-of-possible-implementation-for-the-project)
    - [HyperDB](#hyperdb)
    - [VectorDB](#vectordb)
## Vector Database
A vector database is a database that store vectors of data. These vectors are usually generated by a [Vector Embedding](#vector-embeddings) mechanism that translates data into it's representative vector (usually in numerical values). The common pipeline of a vector database is:
1. Indexing, the vector database indexes the vectors using an algorithm, to maps the vectors to a data structure for faster searching. Index and the vectors then saved into the vector database ready to be queried.
2. Querying, happens when someone wanted to fetch an information by sending a query. The query then translated into an indexed query vector, then all the vectors stored are searched by using nearest-neighbors algorithm.
3. Post Processing, the database retrieves the final nearest neighbors from the dataset, post-processes, and returns the final results. It include re-ranking the nearest neighbors using different similarity algorithm.

Some of vector databases available and open-sourced are:
- [Chroma](https://www.trychroma.com/)
- [FAISS](https://github.com/facebookresearch/faiss)
- [Weaviate](https://weaviate.io/)
- [Qdrant](https://qdrant.tech/)
  
## Vector Embeddings
Vector Embeddings are numerical representations of data that captures semantic relationships and similarities, making it possible to be used in mathematical operations and comparison. There are many existing vector embeddings algorithm and codes, usually built for specific use case. For example, Word2Vec, maps words with other words to find its relation by learning texts that consists the words.
  
There are many types of embeddings, differentiated by the media it process. Some of them are:
1. Text Embeddings, the easiest one to understand and used in most NLP models. It structures a text based on its relation to other words.
2. Image Embeddings, representing multiple features of an image. It can be full images, individual pixels, a group of pixels, filtered images, and so on.
3. Document Embeddings, extends text embeddings by capturing the overall semantic meaning of the whole document, make it possible to do tasks like document classification, clustering, or information retrieval.

## FAISS
FAISS is Facebook AI Similarity Search, developed by Facebook AI that enables efficient similarity search. It is open-source and can be used directly to Python codes. It provides several distance algorithm when building vectors and finding relations between the data points. One of them is L2 (Euclidean) distance, an algorithm that is very accurate in finding relation between the point that we want to search (in this term, a query) with the whole data in the vector space. Below is L2 distance in mathematical expression, y is data point of the database, q is the query. This distance is very time consuming and heavy load if the data saved in the vector space is big.

$d = \sqrt{\sum_{i=1}^{k} (y_i - q_i)^2}$

To enhance the similarity search, FAISS use partitioning of the vector spaces. By partitioning, the query is processed only to the closest partition only, not the whole vector space, make it faster to be processed.

For more integration of FAISS can be found here: [FAISS Tutorial](https://www.pinecone.io/learn/series/faiss/faiss-tutorial/).

## List of Possible Implementation for the Project
### HyperDB
Documentation: https://github.com/jdagdelen/hyperDB/tree/main

HyperDB is a hyper-fast local vector database for use with LLM Agents. It is available in pythonic, to install the package from pyPI:

``pip install hyperdb-python``

To use, we can call ``save()`` to export embedded data into a file, and `load()` to import data back from the file. To query a data, can use `query()`. Here is a simple usage from the HyperDB documentation:

```
import json
from hyperdb import HyperDB

# Load documents from the JSONL file
documents = []

with open("demo/pokemon.jsonl", "r") as f:
    for line in f:
        documents.append(json.loads(line))

# Instantiate HyperDB with the list of documents
db = HyperDB(documents, key="info.description")

# Save the HyperDB instance to a file
db.save("demo/pokemon_hyperdb.pickle.gz")

# Load the HyperDB instance from the save file
db.load("demo/pokemon_hyperdb.pickle.gz")

# Query the HyperDB instance with a text input
results = db.query("Likes to sleep.", top_k=5)
```

### VectorDB
Documentation: https://github.com/kagisearch/vectordb 

another vector database that is simple, lightweight, fully local, end-to-end solution for using embeddings-based text retrieval. It is also available to be installed using pip:

``pip install vectordb2``

Here is the usage example:
```
from vectordb import Memory

# Memory is where all content you want to store/search goes.
memory = Memory()

memory.save(
    ["apples are green", "oranges are orange"],  # save your text content. for long text we will automatically chunk it
    [{"url": "https://apples.com"}, {"url": "https://oranges.com"}], # associate any kind of metadata with it (optional)
)

# Search for top n relevant results, automatically using embeddings
query = "green"
results = memory.search(query, top_n = 1)

print(results)
```
It returns the data, metadata, and its vector distance against the query like this:
```
[
  {
    "chunk": "apples are green",
    "metadata": {"url": "https://apples.com"},
    "distance": 0.87
  }
]
```