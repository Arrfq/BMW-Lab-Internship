{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# Parameter windowing\n",
    "input_width = 60*4\n",
    "label_width = 60\n",
    "shift = label_width\n",
    "total_window_size = input_width + shift\n",
    "OUT_STEPS = label_width\n",
    "# Definisikan irisan untuk input dan label\n",
    "input_slice = slice(0, input_width)\n",
    "label_start = total_window_size - label_width\n",
    "labels_slice = slice(label_start, None)\n",
    "train_df = None\n",
    "output_selected=['RRU.PrbUsedDl']\n",
    "train_name_cells=['S1/B2/C1']\n",
    "test_name_cells=['S7/B2/C1']\n",
    "file_path=\"D:\\\\KULIAH\\\\teep\\\\AI\\\\dataset\\\\08_01_2024\\\\CellReports.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_coding(timestamps, max_values):\n",
    "    sin_features = np.sin(2 * np.pi * timestamps / max_values)\n",
    "    cos_features = np.cos(2 * np.pi * timestamps / max_values)\n",
    "    periodic_features = np.concatenate([sin_features, cos_features], axis=-1)\n",
    "    return periodic_features\n",
    "\n",
    "def  preprocess_data(file_path):\n",
    "    df_1=pd.read_csv(file_path)\n",
    "    convert_time=pd.to_datetime(df_1['timestamp'], unit='ms',origin='unix')\n",
    "    df_1.insert(df_1.columns.get_loc('timestamp') + 1, 'datetime_column', convert_time)\n",
    "    df_1.insert(df_1.columns.get_loc('datetime_column') + 2, 'hour', df_1['datetime_column'].dt.hour+df_1['datetime_column'].dt.minute/60)\n",
    "    df_1.set_index('datetime_column', inplace=True)\n",
    "    df_1.drop(columns=['timestamp'], inplace=True)\n",
    "    df_1['sin_time'] = np.sin(df_1['hour'] * (2 * np.pi / 24))\n",
    "    df_1['cos_time'] = np.cos(df_1['hour']* (2 * np.pi / 24))\n",
    "    seleted_columns = ['Viavi.Cell.Name','RRU.PrbUsedDl', 'sin_time', 'cos_time'] \n",
    "    df_2= df_1[seleted_columns].copy()\n",
    "    cell_name= train_name_cells+test_name_cells\n",
    "    df= df_2[df_2['Viavi.Cell.Name'].isin(cell_name)].copy()\n",
    "    \n",
    "    return df, cell_name\n",
    "\n",
    "def make_windows(data_x,data_y, total_window_size, input_slice, labels_slice):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data_x) - total_window_size + 1):\n",
    "        window_x = data_x[i:i+total_window_size]\n",
    "        x.append(window_x[input_slice])\n",
    "\n",
    "    for i in range(len(data_y) - total_window_size + 1):\n",
    "        window_y= data_y[i:i+total_window_size]\n",
    "        y.append(window_y[labels_slice])\n",
    "\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "def standardize_data(data, train_df, isoutput=True, column_output=output_selected):\n",
    "    if isoutput:\n",
    "        median = train_df[column_output].median().values\n",
    "        q1 = train_df[column_output].quantile(0.25).values\n",
    "        q3 = train_df[column_output].quantile(0.75).values\n",
    "    else:\n",
    "        median = train_df.median().values\n",
    "        q1 = train_df.quantile(0.25).values\n",
    "        q3 = train_df.quantile(0.75).values\n",
    "\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Reshape for broadcasting with 2D matrix\n",
    "    median = median.reshape(1, -1)\n",
    "    iqr = iqr.reshape(1, -1)\n",
    "\n",
    "    return (data - median) / iqr\n",
    "\n",
    "def inverse_standardize_data(data, train_df, isoutput=True, column_output=output_selected):\n",
    "    if isoutput:\n",
    "        median = train_df[column_output].median().values\n",
    "        q1 = train_df[column_output].quantile(0.25).values\n",
    "        q3 = train_df[column_output].quantile(0.75).values\n",
    "    else:\n",
    "        median = train_df.median().values\n",
    "        q1 = train_df.quantile(0.25).values\n",
    "        q3 = train_df.quantile(0.75).values\n",
    "\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Reshape for broadcasting with 2D matrix\n",
    "    median = median.reshape(1, -1)\n",
    "    iqr = iqr.reshape(1, -1)\n",
    "\n",
    "    return data * iqr + median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GAME\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "d:\\GAME\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import time\n",
    "import tensorflow_addons as tfa\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='tensorflow_addons')\n",
    "\n",
    "# Define the PinballLoss function\n",
    "pinball_loss = tfa.losses.PinballLoss(tau=0.5, reduction=tf.keras.losses.Reduction.AUTO, name='pinball_loss')\n",
    "\n",
    "def tensorflow_cnn(X_train_scaled, Y_train_scaled, X_validation_scaled, Y_validation_scaled, \n",
    "                    learning_rate, target_error, max_epochs, max_sampel_batch,\n",
    "                    patience, save_best_model_path, validation_data=False, load_model=None, out_steps=OUT_STEPS):\n",
    "    global model\n",
    "\n",
    "    class MAEStopCallback(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, threshold):\n",
    "            super(MAEStopCallback, self).__init__()\n",
    "            self.threshold = threshold\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if logs['mae'] < self.threshold:\n",
    "                print(f\"\\nMAE reached below {self.threshold}. Stopping training.\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "                \n",
    "    input_width = X_train_scaled.shape[1]\n",
    "    CONV_WIDTH = input_width # Define the width of the convolutional window\n",
    "    num_features = X_train_scaled.shape[2]\n",
    "    num_output = Y_train_scaled.shape[2]\n",
    "    out_steps = out_steps\n",
    "\n",
    "    if load_model is None:\n",
    "        print(\"Create new model\")\n",
    "\n",
    "        # Define the model\n",
    "        inputs = tf.keras.Input(shape=(input_width, num_features))\n",
    "\n",
    "        # Initial Conv1D layer\n",
    "        layer_first = tf.keras.layers.Conv1D(filters=64, kernel_size=8, dilation_rate=1, \n",
    "                                            activation='relu', padding='same')(inputs)\n",
    "\n",
    "        # Residual connection initialization\n",
    "        residual_before = tf.keras.layers.Conv1D(filters=24, kernel_size=8, dilation_rate=1, activation='relu', padding='same')(layer_first)\n",
    "\n",
    "        # Residual block with multiple dilations\n",
    "        for _ in range(8):\n",
    "            for dilation in (1, 2, 4):\n",
    "                residual = tf.keras.layers.Conv1D(filters=24, kernel_size=8, dilation_rate=dilation, activation='relu', padding='same')(residual_before)\n",
    "                residual = tf.keras.layers.BatchNormalization()(residual)\n",
    "                residual = tf.keras.layers.Dropout(0.05)(residual)\n",
    "                residual = tf.keras.layers.Add()([residual_before, residual])\n",
    "                residual_before = residual\n",
    "\n",
    "        # Output layer\n",
    "        last_cnn= tf.keras.layers.Conv1D(filters=60, kernel_size=input_width, dilation_rate=1, activation='linear', name='last_cnn')(residual_before)\n",
    "        output = tf.keras.layers.Dense(out_steps * num_output, name='output_layer')(last_cnn)\n",
    "        output=tf.keras.layers.Reshape([out_steps, num_output])(output)\n",
    "        # Define the model\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "        model.summary()\n",
    "    else:\n",
    "        print(\"Load model\")\n",
    "        model = tf.keras.models.load_model(load_model)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,amsgrad=True)\n",
    "    model.compile(optimizer=optimizer, \n",
    "              loss=pinball_loss, \n",
    "              metrics=['mse', 'mae', 'mape', tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "    mae_stop_callback = MAEStopCallback(threshold=target_error)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        save_best_model_path,\n",
    "        monitor='val_loss',     \n",
    "        mode='min',         \n",
    "        save_best_only=True, \n",
    "        verbose=1            \n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_loss',     \n",
    "        mode='min',         \n",
    "        patience=patience,    \n",
    "        restore_best_weights=True,\n",
    "        verbose=1            \n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.3, patience=int(1), min_lr=0.00001, verbose=1)\n",
    "\n",
    "    time_start = time.time()\n",
    "    if validation_data:\n",
    "        model.fit(X_train_scaled, Y_train_scaled, epochs=max_epochs, batch_size=max_sampel_batch,  \n",
    "                  callbacks=[mae_stop_callback, checkpoint_callback, early_stopping_callback, reduce_lr], \n",
    "                  validation_data=(X_validation_scaled, Y_validation_scaled), validation_batch_size=max_sampel_batch)\n",
    "    else:\n",
    "        model.fit(X_train_scaled, Y_train_scaled, epochs=max_epochs, batch_size=max_sampel_batch, \n",
    "                  callbacks=[mae_stop_callback, checkpoint_callback, early_stopping_callback, reduce_lr])\n",
    "    \n",
    "    print(\"time computation seconds: \", time.time() - time_start)\n",
    "    \n",
    "    loss, MSE, MAE, RMSE, MAPE = model.evaluate(X_train_scaled, Y_train_scaled)\n",
    "    print(\"loss: \", loss, \"MSE: \", MSE, \"MAE: \", MAE, \"RMSE: \", RMSE, \"MAPE: \", MAPE)\n",
    "    \n",
    "    return model, loss, MSE, MAE, RMSE, MAPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_program(train_df,val_df, index_cell, name_file, name_file_before):\n",
    "    train_scaled = standardize_data(train_df, isoutput=False, train_df=train_df)\n",
    "    val_scaled= standardize_data(val_df, isoutput=False, train_df=train_df)\n",
    "    # Membuat windowed dataset untuk set pelatihan, validasi, dan pengujian\n",
    "\n",
    "    x_train_scaled, y_train_scaled = make_windows(train_scaled.to_numpy(), train_scaled[output_selected].to_numpy(),total_window_size, input_slice, labels_slice)\n",
    "    x_val_scaled, y_val_scaled = make_windows(val_scaled.to_numpy(), val_scaled[output_selected].to_numpy(),total_window_size, input_slice, labels_slice)\n",
    "    model, loss, MSE, MAE, RMSE,  MAPE  = tensorflow_cnn(x_train_scaled, y_train_scaled, x_val_scaled, y_val_scaled,\n",
    "                                                        learning_rate=0.001, target_error=0.001,  max_epochs=100, max_sampel_batch=128, \n",
    "                                                        patience=30,  save_best_model_path = name_file, \n",
    "                                                        validation_data=True, load_model=\"TCSM_CNN.hdf5\", out_steps=OUT_STEPS)\n",
    "    model.save(\"TCSM_CNN22.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "name_file:  2hour_cekpoint22_1.hdf5\n",
      "name_file_before:  2hour_cekpoint22_0.hdf5\n",
      "Cell Name:  S1/B2/C1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RRU.PrbUsedDl</th>\n",
       "      <th>sin_time</th>\n",
       "      <th>cos_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime_column</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-08-01 00:00:00</th>\n",
       "      <td>35.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-01 00:01:00</th>\n",
       "      <td>20.116667</td>\n",
       "      <td>0.004363</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-01 00:02:00</th>\n",
       "      <td>30.800000</td>\n",
       "      <td>0.008727</td>\n",
       "      <td>0.999962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-01 00:03:00</th>\n",
       "      <td>34.450000</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>0.999914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-01 00:04:00</th>\n",
       "      <td>25.900000</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.999848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-08 23:55:00</th>\n",
       "      <td>25.116667</td>\n",
       "      <td>-0.021815</td>\n",
       "      <td>0.999762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-08 23:56:00</th>\n",
       "      <td>24.666667</td>\n",
       "      <td>-0.017452</td>\n",
       "      <td>0.999848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-08 23:57:00</th>\n",
       "      <td>59.133333</td>\n",
       "      <td>-0.013090</td>\n",
       "      <td>0.999914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-08 23:58:00</th>\n",
       "      <td>6.583333</td>\n",
       "      <td>-0.008727</td>\n",
       "      <td>0.999962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-08 23:59:00</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>-0.004363</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11520 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     RRU.PrbUsedDl  sin_time  cos_time\n",
       "datetime_column                                       \n",
       "2024-08-01 00:00:00      35.200000  0.000000  1.000000\n",
       "2024-08-01 00:01:00      20.116667  0.004363  0.999990\n",
       "2024-08-01 00:02:00      30.800000  0.008727  0.999962\n",
       "2024-08-01 00:03:00      34.450000  0.013090  0.999914\n",
       "2024-08-01 00:04:00      25.900000  0.017452  0.999848\n",
       "...                            ...       ...       ...\n",
       "2024-08-08 23:55:00      25.116667 -0.021815  0.999762\n",
       "2024-08-08 23:56:00      24.666667 -0.017452  0.999848\n",
       "2024-08-08 23:57:00      59.133333 -0.013090  0.999914\n",
       "2024-08-08 23:58:00       6.583333 -0.008727  0.999962\n",
       "2024-08-08 23:59:00       5.000000 -0.004363  0.999990\n",
       "\n",
       "[11520 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model\n",
      "Epoch 1/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.2253 - mse: 0.3214 - mae: 0.4505 - mape: 89051.0781 - rmse: 0.5669\n",
      "Epoch 1: val_loss improved from inf to 0.31987, saving model to 2hour_cekpoint22_1.hdf5\n",
      "88/88 [==============================] - 14s 75ms/step - loss: 0.2253 - mse: 0.3214 - mae: 0.4505 - mape: 89051.0781 - rmse: 0.5669 - val_loss: 0.3199 - val_mse: 0.6082 - val_mae: 0.6397 - val_mape: 71947.2266 - val_rmse: 0.7799 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.2118 - mse: 0.2880 - mae: 0.4236 - mape: 74712.3281 - rmse: 0.5367\n",
      "Epoch 2: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 6s 68ms/step - loss: 0.2118 - mse: 0.2880 - mae: 0.4237 - mape: 74364.0547 - rmse: 0.5367 - val_loss: 0.3492 - val_mse: 0.7379 - val_mae: 0.6984 - val_mape: 116166.9766 - val_rmse: 0.8590 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.2034 - mse: 0.2693 - mae: 0.4068 - mape: 77064.0312 - rmse: 0.5189\n",
      "Epoch 3: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 0.2034 - mse: 0.2693 - mae: 0.4068 - mape: 77064.0312 - rmse: 0.5189 - val_loss: 0.4009 - val_mse: 0.9257 - val_mae: 0.8019 - val_mape: 140808.4219 - val_rmse: 0.9621 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1972 - mse: 0.2562 - mae: 0.3945 - mape: 77302.6094 - rmse: 0.5061\n",
      "Epoch 4: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 9s 105ms/step - loss: 0.1972 - mse: 0.2562 - mae: 0.3945 - mape: 77302.6094 - rmse: 0.5061 - val_loss: 0.3903 - val_mse: 0.8628 - val_mae: 0.7807 - val_mape: 132435.4375 - val_rmse: 0.9289 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1935 - mse: 0.2484 - mae: 0.3869 - mape: 79966.2344 - rmse: 0.4984\n",
      "Epoch 5: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 10s 112ms/step - loss: 0.1935 - mse: 0.2484 - mae: 0.3869 - mape: 79966.2344 - rmse: 0.4984 - val_loss: 0.4058 - val_mse: 0.9195 - val_mae: 0.8115 - val_mape: 121943.2344 - val_rmse: 0.9589 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1900 - mse: 0.2412 - mae: 0.3799 - mape: 78330.0625 - rmse: 0.4912\n",
      "Epoch 6: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 0.1900 - mse: 0.2412 - mae: 0.3799 - mape: 78330.0625 - rmse: 0.4912 - val_loss: 0.4037 - val_mse: 0.9079 - val_mae: 0.8074 - val_mape: 131148.4688 - val_rmse: 0.9529 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1873 - mse: 0.2359 - mae: 0.3746 - mape: 74908.5156 - rmse: 0.4857\n",
      "Epoch 7: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 0.1873 - mse: 0.2359 - mae: 0.3746 - mape: 74908.5156 - rmse: 0.4857 - val_loss: 0.4038 - val_mse: 0.9187 - val_mae: 0.8076 - val_mape: 136569.1094 - val_rmse: 0.9585 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1851 - mse: 0.2317 - mae: 0.3702 - mape: 73978.7969 - rmse: 0.4814\n",
      "Epoch 8: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 10s 110ms/step - loss: 0.1851 - mse: 0.2317 - mae: 0.3702 - mape: 73978.7969 - rmse: 0.4814 - val_loss: 0.3900 - val_mse: 0.8647 - val_mae: 0.7801 - val_mape: 123826.9609 - val_rmse: 0.9299 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1834 - mse: 0.2289 - mae: 0.3669 - mape: 68479.9141 - rmse: 0.4784\n",
      "Epoch 9: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 10s 118ms/step - loss: 0.1834 - mse: 0.2289 - mae: 0.3669 - mape: 68479.9141 - rmse: 0.4784 - val_loss: 0.4124 - val_mse: 0.9313 - val_mae: 0.8248 - val_mape: 115086.5625 - val_rmse: 0.9650 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.1826 - mse: 0.2271 - mae: 0.3651 - mape: 73085.9609 - rmse: 0.4765\n",
      "Epoch 10: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 7s 83ms/step - loss: 0.1826 - mse: 0.2271 - mae: 0.3652 - mape: 72681.2344 - rmse: 0.4766 - val_loss: 0.4011 - val_mse: 0.8969 - val_mae: 0.8022 - val_mape: 123989.9453 - val_rmse: 0.9470 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.1806 - mse: 0.2235 - mae: 0.3613 - mape: 69098.5703 - rmse: 0.4728\n",
      "Epoch 11: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 8s 87ms/step - loss: 0.1806 - mse: 0.2235 - mae: 0.3612 - mape: 68920.6094 - rmse: 0.4727 - val_loss: 0.4117 - val_mse: 0.9299 - val_mae: 0.8234 - val_mape: 108631.0391 - val_rmse: 0.9643 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.1791 - mse: 0.2207 - mae: 0.3581 - mape: 70308.2578 - rmse: 0.4698\n",
      "Epoch 12: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 0.1791 - mse: 0.2207 - mae: 0.3581 - mape: 70307.9688 - rmse: 0.4698 - val_loss: 0.4243 - val_mse: 0.9802 - val_mae: 0.8486 - val_mape: 105160.1094 - val_rmse: 0.9900 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.1778 - mse: 0.2184 - mae: 0.3556 - mape: 70826.7656 - rmse: 0.4673\n",
      "Epoch 13: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 8s 93ms/step - loss: 0.1778 - mse: 0.2183 - mae: 0.3555 - mape: 71352.4766 - rmse: 0.4672 - val_loss: 0.4097 - val_mse: 0.9259 - val_mae: 0.8194 - val_mape: 106274.0625 - val_rmse: 0.9622 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.1762 - mse: 0.2155 - mae: 0.3523 - mape: 70661.4844 - rmse: 0.4642\n",
      "Epoch 14: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 9s 97ms/step - loss: 0.1761 - mse: 0.2153 - mae: 0.3522 - mape: 70318.9141 - rmse: 0.4641 - val_loss: 0.4067 - val_mse: 0.9100 - val_mae: 0.8134 - val_mape: 100681.7812 - val_rmse: 0.9539 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1749 - mse: 0.2134 - mae: 0.3498 - mape: 74250.8750 - rmse: 0.4619\n",
      "Epoch 15: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 8s 96ms/step - loss: 0.1749 - mse: 0.2134 - mae: 0.3498 - mape: 74250.8750 - rmse: 0.4619 - val_loss: 0.3943 - val_mse: 0.8632 - val_mae: 0.7885 - val_mape: 105662.3359 - val_rmse: 0.9291 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.1737 - mse: 0.2112 - mae: 0.3474 - mape: 64681.3008 - rmse: 0.4596\n",
      "Epoch 16: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 8s 96ms/step - loss: 0.1737 - mse: 0.2112 - mae: 0.3474 - mape: 64668.4766 - rmse: 0.4596 - val_loss: 0.4203 - val_mse: 0.9627 - val_mae: 0.8405 - val_mape: 96267.7344 - val_rmse: 0.9812 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1723 - mse: 0.2089 - mae: 0.3447 - mape: 62189.4297 - rmse: 0.4570\n",
      "Epoch 17: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 10s 112ms/step - loss: 0.1723 - mse: 0.2089 - mae: 0.3447 - mape: 62189.4297 - rmse: 0.4570 - val_loss: 0.4423 - val_mse: 1.0456 - val_mae: 0.8847 - val_mape: 94612.9062 - val_rmse: 1.0225 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1710 - mse: 0.2063 - mae: 0.3419 - mape: 65619.2266 - rmse: 0.4542\n",
      "Epoch 18: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 10s 115ms/step - loss: 0.1710 - mse: 0.2063 - mae: 0.3419 - mape: 65619.2266 - rmse: 0.4542 - val_loss: 0.4217 - val_mse: 0.9567 - val_mae: 0.8435 - val_mape: 82542.4922 - val_rmse: 0.9781 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1697 - mse: 0.2038 - mae: 0.3393 - mape: 67808.5781 - rmse: 0.4514\n",
      "Epoch 19: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 10s 113ms/step - loss: 0.1697 - mse: 0.2038 - mae: 0.3393 - mape: 67808.5781 - rmse: 0.4514 - val_loss: 0.4459 - val_mse: 1.0556 - val_mae: 0.8918 - val_mape: 76806.8906 - val_rmse: 1.0274 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1677 - mse: 0.2003 - mae: 0.3353 - mape: 70458.3438 - rmse: 0.4475\n",
      "Epoch 20: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 0.1677 - mse: 0.2003 - mae: 0.3353 - mape: 70458.3438 - rmse: 0.4475 - val_loss: 0.4212 - val_mse: 0.9594 - val_mae: 0.8424 - val_mape: 73651.8516 - val_rmse: 0.9795 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1654 - mse: 0.1962 - mae: 0.3308 - mape: 68650.7656 - rmse: 0.4430\n",
      "Epoch 21: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 10s 109ms/step - loss: 0.1654 - mse: 0.1962 - mae: 0.3308 - mape: 68650.7656 - rmse: 0.4430 - val_loss: 0.4120 - val_mse: 0.9283 - val_mae: 0.8241 - val_mape: 75772.1562 - val_rmse: 0.9635 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1636 - mse: 0.1928 - mae: 0.3273 - mape: 68060.3125 - rmse: 0.4391\n",
      "Epoch 22: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 10s 110ms/step - loss: 0.1636 - mse: 0.1928 - mae: 0.3273 - mape: 68060.3125 - rmse: 0.4391 - val_loss: 0.4000 - val_mse: 0.8869 - val_mae: 0.8000 - val_mape: 57737.8984 - val_rmse: 0.9418 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1620 - mse: 0.1897 - mae: 0.3240 - mape: 75821.7266 - rmse: 0.4356\n",
      "Epoch 23: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 9s 104ms/step - loss: 0.1620 - mse: 0.1897 - mae: 0.3240 - mape: 75821.7266 - rmse: 0.4356 - val_loss: 0.4294 - val_mse: 0.9953 - val_mae: 0.8587 - val_mape: 72531.1484 - val_rmse: 0.9977 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1597 - mse: 0.1858 - mae: 0.3195 - mape: 70620.2422 - rmse: 0.4311\n",
      "Epoch 24: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 9s 107ms/step - loss: 0.1597 - mse: 0.1858 - mae: 0.3195 - mape: 70620.2422 - rmse: 0.4311 - val_loss: 0.4361 - val_mse: 1.0233 - val_mae: 0.8722 - val_mape: 58007.6016 - val_rmse: 1.0116 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1577 - mse: 0.1820 - mae: 0.3154 - mape: 75261.4922 - rmse: 0.4266\n",
      "Epoch 25: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 10s 115ms/step - loss: 0.1577 - mse: 0.1820 - mae: 0.3154 - mape: 75261.4922 - rmse: 0.4266 - val_loss: 0.4084 - val_mse: 0.9325 - val_mae: 0.8168 - val_mape: 74259.2188 - val_rmse: 0.9657 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1561 - mse: 0.1785 - mae: 0.3123 - mape: 76488.6562 - rmse: 0.4225\n",
      "Epoch 26: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 10s 113ms/step - loss: 0.1561 - mse: 0.1785 - mae: 0.3123 - mape: 76488.6562 - rmse: 0.4225 - val_loss: 0.4345 - val_mse: 1.0198 - val_mae: 0.8690 - val_mape: 56703.5508 - val_rmse: 1.0099 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1533 - mse: 0.1724 - mae: 0.3067 - mape: 81380.7266 - rmse: 0.4152\n",
      "Epoch 27: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 10s 116ms/step - loss: 0.1533 - mse: 0.1724 - mae: 0.3067 - mape: 81380.7266 - rmse: 0.4152 - val_loss: 0.4281 - val_mse: 0.9998 - val_mae: 0.8563 - val_mape: 59384.7656 - val_rmse: 0.9999 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.1502 - mse: 0.1654 - mae: 0.3005 - mape: 88899.3125 - rmse: 0.4067\n",
      "Epoch 28: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 8s 89ms/step - loss: 0.1502 - mse: 0.1654 - mae: 0.3005 - mape: 89568.3516 - rmse: 0.4067 - val_loss: 0.4084 - val_mse: 0.9191 - val_mae: 0.8167 - val_mape: 40516.8281 - val_rmse: 0.9587 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.1473 - mse: 0.1589 - mae: 0.2947 - mape: 85475.9375 - rmse: 0.3986\n",
      "Epoch 29: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 8s 88ms/step - loss: 0.1474 - mse: 0.1590 - mae: 0.2947 - mape: 86111.9297 - rmse: 0.3987 - val_loss: 0.4465 - val_mse: 1.0685 - val_mae: 0.8930 - val_mape: 46593.6602 - val_rmse: 1.0337 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.1444 - mse: 0.1530 - mae: 0.2889 - mape: 74908.8516 - rmse: 0.3911\n",
      "Epoch 30: val_loss did not improve from 0.31987\n",
      "88/88 [==============================] - 8s 91ms/step - loss: 0.1444 - mse: 0.1530 - mae: 0.2889 - mape: 74583.7266 - rmse: 0.3911 - val_loss: 0.4459 - val_mse: 1.0621 - val_mae: 0.8919 - val_mape: 54172.2969 - val_rmse: 1.0306 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1421 - mse: 0.1482 - mae: 0.2841 - mape: 72974.1953 - rmse: 0.3849\n",
      "Epoch 31: val_loss did not improve from 0.31987\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "88/88 [==============================] - 8s 96ms/step - loss: 0.1421 - mse: 0.1482 - mae: 0.2841 - mape: 72974.1953 - rmse: 0.3849 - val_loss: 0.4199 - val_mse: 0.9561 - val_mae: 0.8397 - val_mape: 53025.7578 - val_rmse: 0.9778 - lr: 0.0010\n",
      "Epoch 31: early stopping\n",
      "time computation seconds:  281.8287887573242\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.2140 - mse: 0.2946 - mae: 0.4279 - mape: 81322.0391 - rmse: 0.5428\n",
      "loss:  0.21395136415958405 MSE:  0.29461970925331116 MAE:  0.4279025197029114 RMSE:  81322.0390625 MAPE:  0.5427888035774231\n"
     ]
    }
   ],
   "source": [
    "df_start, cell_name= preprocess_data(\"D:\\\\KULIAH\\\\teep\\\\AI\\\\dataset\\\\08_01_2024\\\\CellReports.csv\")\n",
    "#now = datetime.datetime.now()\n",
    "timestamp = \"cekpoint22\"\n",
    "for index in range(0,1):\n",
    "    print(index)\n",
    "\n",
    "    train_df = df_start[df_start['Viavi.Cell.Name'] == train_name_cells[index]]\n",
    "    train_df = train_df.loc[~train_df.index.duplicated()]\n",
    "    train_df=train_df.drop(columns=['Viavi.Cell.Name']).astype(float).copy()\n",
    "\n",
    "    val_df= df_start[df_start['Viavi.Cell.Name'] == test_name_cells[0]]\n",
    "    val_df=val_df.loc[~val_df.index.duplicated()]\n",
    "    val_df=val_df.drop(columns=['Viavi.Cell.Name']).astype(float).copy()\n",
    "\n",
    "    name_file='2hour_%s_%s.hdf5'%(timestamp, index+1)\n",
    "    name_file_before='2hour_%s_%s.hdf5'%(timestamp, index)\n",
    "    print(\"name_file: \", name_file)\n",
    "    print(\"name_file_before: \", name_file_before)\n",
    "    print(\"Cell Name: \", cell_name[index])\n",
    "    display(train_df)\n",
    "    running_program(train_df=train_df, val_df=val_df, index_cell=index, name_file=name_file, name_file_before=name_file_before)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
